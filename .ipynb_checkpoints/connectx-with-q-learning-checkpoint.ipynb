{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Q-Learning algorithm\n",
    "> \"Q-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\"\n",
    "[*wiki*](https://en.wikipedia.org/wiki/Q-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ToC\"></a>\n",
    "# Table of Contents\n",
    "1. [Install libraries](#install_libraries)\n",
    "1. [Import libraries](#import_libraries)\n",
    "1. [Define useful classes](#define_useful_classes)\n",
    "1. [Create ConnectX environment](#create_connectx_environment)\n",
    "1. [Configure hyper-parameters](#configure_hyper_parameters)\n",
    "1. [Train the agent](#train_the_agent)\n",
    "1. [Create an agent](#create_an_agent)\n",
    "1. [Evaluate the agent](#evaluate_the_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"install_libraries\"></a>\n",
    "# Install libraries\n",
    "[Back to Table of Contents](#ToC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'kaggle-environments>=0.1.6' > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import_libraries\"></a>\n",
    "# Import libraries\n",
    "[Back to Table of Contents](#ToC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "from tqdm.notebook import tqdm\n",
    "from kaggle_environments import evaluate, make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"define_useful_classes\"></a>\n",
    "# Define useful classes\n",
    "NOTE: It's not easy to generate a Q-Table with all possible states; and even if I can do so, the huge number of states will cost much of memory. So, I use the approach that dynamically adding newly discovered states into an object of QTable class created below.\n",
    "\n",
    "[Back to Table of Contents](#ToC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class ConnectX(gym.Env):\n",
    "    def __init__(self, switch_prob=0.5):\n",
    "        self.env = make('connectx', debug=True)\n",
    "        self.pair = [None, 'negamax']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "        self.switch_prob = switch_prob\n",
    "        \n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "\n",
    "    def switch_trainer(self):\n",
    "        self.pair = self.pair[::-1]\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        if random.uniform(0, 1) < self.switch_prob:\n",
    "            self.switch_trainer()\n",
    "        return self.trainer.reset()\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)\n",
    "\n",
    "\n",
    "class QTable:\n",
    "    def __init__(self, action_space):\n",
    "        self.table = dict()\n",
    "        self.action_space = action_space\n",
    "        \n",
    "    def add_item(self, state_key):\n",
    "        self.table[state_key] = list(np.zeros(self.action_space.n))\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        board = state.board[:] # Get a copy\n",
    "        board.append(state.mark)\n",
    "        state_key = np.array(board).astype(str)\n",
    "        state_key = hex(int(''.join(state_key), 3))[2:]\n",
    "        if state_key not in self.table.keys():\n",
    "            self.add_item(state_key)\n",
    "        \n",
    "        return self.table[state_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"create_connectx_environment\"></a>\n",
    "# Create ConnectX environment\n",
    "[Back to Table of Contents](#ToC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ConnectX()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"configure_hyper_parameters\"></a>\n",
    "# Configure hyper-parameters\n",
    "[Back to Table of Contents](#ToC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.99\n",
    "min_epsilon = 0.1\n",
    "\n",
    "episodes = 10000\n",
    "\n",
    "alpha_decay_step = 1000\n",
    "alpha_decay_rate = 0.9\n",
    "epsilon_decay_rate = 0.9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"train_the_agent\"></a>\n",
    "# Train the agent\n",
    "[Back to Table of Contents](#ToC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddabcfb686e84e3aaee0531e6dd3afe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Struct' object has no attribute 'board'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f5f571c4ede7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f5f571c4ede7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Struct' object has no attribute 'board'"
     ]
    }
   ],
   "source": [
    "q_table = QTable(env.action_space)\n",
    "\n",
    "all_epochs = []\n",
    "all_total_rewards = []\n",
    "all_avg_rewards = [] # Last 100 steps\n",
    "all_qtable_rows = []\n",
    "all_epsilons = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    state = env.reset()\n",
    "\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)\n",
    "    epochs, total_rewards = 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = choice([c for c in range(env.action_space.n) if state.board[c] == 0])\n",
    "        else:\n",
    "            row = q_table(state)[:]\n",
    "            selected_items = []\n",
    "            for j in range(env.action_space.n):\n",
    "                if state.board[j] == 0:\n",
    "                    selected_items.append(row[j])\n",
    "                else:\n",
    "                    selected_items.append(-1e7)\n",
    "            action = int(np.argmax(selected_items))\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Apply new rules\n",
    "        if done:\n",
    "            if reward == 1: # Won\n",
    "                reward = 20\n",
    "            elif reward == 0: # Lost\n",
    "                reward = -20\n",
    "            else: # Draw\n",
    "                reward = 10\n",
    "        else:\n",
    "            reward = -0.05 # Try to prevent the agent from taking a long move\n",
    "\n",
    "        old_value = q_table(state)[action]\n",
    "        next_max = np.max(q_table(next_state))\n",
    "        \n",
    "        # Update Q-value\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table(state)[action] = new_value\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        total_rewards += reward\n",
    "\n",
    "\n",
    "    all_epochs.append(epochs)\n",
    "    all_total_rewards.append(total_rewards)\n",
    "    avg_rewards = np.mean(all_total_rewards[max(0, i-100):(i+1)])\n",
    "    all_avg_rewards.append(avg_rewards)\n",
    "    all_qtable_rows.append(len(q_table.table))\n",
    "    all_epsilons.append(epsilon)\n",
    "\n",
    "    if (i+1) % alpha_decay_step == 0:\n",
    "        alpha *= alpha_decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q_table.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# for k in q_table.table.keys():\n",
    "#     print('State:', k)\n",
    "#     print('Action-Value:', list(q_table.table[k]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(all_epochs)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Num of steps')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(all_total_rewards)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Total rewards')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_avg_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Avg rewards (100)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_qtable_rows)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Explored states')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_epsilons)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"create_an_agent\"></a>\n",
    "# Create an Agent\n",
    "[Back to Table of Contents](#ToC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dict_q_table = q_table.table.copy()\n",
    "dict_q_table = dict()\n",
    "\n",
    "for k in tmp_dict_q_table:\n",
    "    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n",
    "        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = '''def my_agent(observation, configuration):\n",
    "    from random import choice\n",
    "\n",
    "    q_table = ''' \\\n",
    "    + str(dict_q_table).replace(' ', '') \\\n",
    "    + '''\n",
    "\n",
    "    board = observation.board[:]\n",
    "    board.append(observation.mark)\n",
    "    state_key = list(map(str, board))\n",
    "    state_key = hex(int(''.join(state_key), 3))[2:]\n",
    "\n",
    "    if state_key not in q_table.keys():\n",
    "        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n",
    "\n",
    "    action = q_table[state_key]\n",
    "\n",
    "    if observation.board[action] != 0:\n",
    "        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n",
    "\n",
    "    return action\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.py', 'w') as f:\n",
    "    f.write(my_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"evaluate_the_agent\"></a>\n",
    "# Evaluate the agent\n",
    "[Back to Table of Contents](#ToC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import my_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n",
    "\n",
    "# Run multiple episodes to estimate agent's performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
